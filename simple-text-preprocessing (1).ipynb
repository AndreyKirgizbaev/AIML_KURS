{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['imdb_master.csv']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom collections import Counter\nimport itertools","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Рассмотрим преобразование текстов в удобоваримый для нейронной сети вид.<br>\nА именно:<br>\n    - Текст разбивается на слова (токенизация, знаки препинания считаются словами)<br>\n    - Слова подсчитываются для формирования ограниченного словаря. Каждому слову сопоставляется определеннный номер в словаре. \n    Редким словам назначается специальный номер (эквивалентно замене редких слов на спец. слово <UNK> (неизвестное слово)). \n    - Последовательности слов преобразуются в последовательности номеров слов. Добавляются спец. слова для обозначения начала и конца текста. \n    - Полученные последовательности выравниваются по заданной максимальной длине через обрезание или дополнение номером спец.символа <PAD>\n"},{"metadata":{},"cell_type":"markdown","source":"Класс для хранения текста в виде последовательности номеров и его закодированной метки"},{"metadata":{"trusted":true},"cell_type":"code","source":"class InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids, label_id):\n        self.input_ids = input_ids\n        self.label_id = label_id","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Класс словаря. Метод word2id возвращает номер слова, id2word - наоборот, восстанавливает слово."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Vocab:\n    def __init__(self, itos, unk_index):\n        self._itos = itos\n        self._stoi = {word:i for i, word in enumerate(itos)}\n        self._unk_index = unk_index\n        \n    def __len__(self):\n        return len(self._itos)\n    \n    def word2id(self, word):\n        idx = self._stoi.get(word)\n        if idx is not None:\n            return idx\n        return self._unk_index\n    \n    def id2word(self, idx):\n        return self._itos[idx]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Интерфейс объекта, преобразующего тексты в последовательности номеров.\ntransform выполняет преобразование при помощи словаря.\nfit_transform выучивает словарь из текста и возвращает такое же преобразование при помощи свежеполученного словаря."},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextToIdsTransformer:\n    def transform():\n        raise NotImplementedError()\n        \n    def fit_transform():\n        raise NotImplementedError()\n\n","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Простая реализация данного интерфейса. Разбиение на слова производится с помощью библиотеки NLTK.\nВ словаре содержатся несколько спец. слов.\nПосле токенизации, к полученной последовательности слов добавляются слева и справа спец. слова для начала и конца текста."},{"metadata":{"trusted":true},"cell_type":"code","source":"class SimpleTextTransformer(TextToIdsTransformer):\n    def __init__(self, max_vocab_size):\n        self.special_words = ['<PAD>', '</UNK>', '<S>', '</S>']\n        self.unk_index = 1\n        self.pad_index = 0\n        self.vocab = None\n        self.max_vocab_size = max_vocab_size\n        \n    def tokenize(self, text):\n        return nltk.tokenize.word_tokenize(text.lower())\n        \n    def build_vocab(self, tokens):\n        itos = []\n        itos.extend(self.special_words)\n        \n        token_counts = Counter(tokens)\n        for word, _ in token_counts.most_common(self.max_vocab_size - len(self.special_words)):\n            itos.append(word)\n            \n        self.vocab = Vocab(itos, self.unk_index)\n    \n    def transform(self, texts):\n        result = []\n        for text in texts:\n            tokens = ['<S>'] + self.tokenize(text) + ['</S>']\n            ids = [self.vocab.word2id(token) for token in tokens]\n            result.append(ids)\n        return result\n    \n    def fit_transform(self, texts):\n        result = []\n        tokenized_texts = [self.tokenize(text) for text in texts]\n        self.build_vocab(itertools.chain(*tokenized_texts))\n        for tokens in tokenized_texts:\n            tokens = ['<S>'] + tokens + ['</S>']\n            ids = [self.vocab.word2id(token) for token in tokens]\n            result.append(ids)\n        return result","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Строим экземпляр входных данных. Обеспечиваем длину последовательности номеров равной max_seq_len. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_features(token_ids, label, max_seq_len, pad_index, label_encoding):\n    if len(token_ids) >= max_seq_len:\n        ids = token_ids[:max_seq_len]\n    else:\n        ids = token_ids + [pad_index for _ in range(max_seq_len - len(token_ids))]\n    return InputFeatures(ids, label_encoding[label])\n        ","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Собираем экземпляры в тензоры"},{"metadata":{"trusted":true},"cell_type":"code","source":"def features_to_tensor(list_of_features):\n    text_tensor = torch.tensor([example.input_ids for example in list_of_features], dtype=torch.long)\n    labels_tensor = torch.tensor([example.label_id for example in list_of_features], dtype=torch.long)\n    return text_tensor, labels_tensor","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb_df = pd.read_csv('../input/imdb_master.csv', encoding='latin-1')\ndev_df = imdb_df[(imdb_df.type == 'train') & (imdb_df.label != 'unsup')]\ntest_df = imdb_df[(imdb_df.type == 'test')]\ntrain_df, val_df = model_selection.train_test_split(dev_df, test_size=0.05, stratify=dev_df.label)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_seq_len=200\nclasses = {'neg': 0, 'pos' : 1}","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text2id = SimpleTextTransformer(10000)\n\ntrain_ids = text2id.fit_transform(train_df['review'])\nval_ids = text2id.transform(val_df['review'])\ntest_ids = text2id.transform(test_df['review'])","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.review.iloc[0][:160])\nprint(train_ids[0][:30])","execution_count":15,"outputs":[{"output_type":"stream","text":"I was seriously looking forward to seeing this film because it seemed truly promising from the coming attractions: Jim Carrey with Godlike powers was an idea th\n[2, 18, 22, 627, 298, 966, 10, 325, 19, 28, 101, 16, 463, 374, 2466, 50, 4, 590, 1, 90, 1146, 3304, 24, 1, 1693, 22, 48, 333, 20, 109]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(train_ids, train_df['label'])]\n\nval_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(val_ids, val_df['label'])]\n\ntest_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(test_ids, test_df['label'])]","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_features[3].input_ids)","execution_count":17,"outputs":[{"output_type":"stream","text":"[2, 827, 4, 260, 5544, 50, 2332, 330, 441, 7, 1164, 272, 17, 42, 1175, 901, 26, 6, 169, 5, 827, 20, 45, 4, 497, 17, 165, 106, 5544, 11, 4055, 7, 2877, 24, 2910, 6, 169, 827, 162, 1448, 229, 7109, 14, 15, 12, 13, 14, 15, 12, 13, 195, 20, 53, 14, 15, 12, 13, 14, 15, 12, 13, 687, 5, 169, 160, 84, 4, 2757, 239, 26, 6, 20, 1717, 31, 56, 81, 100, 315, 52, 4, 1457, 170, 141, 6, 1, 11, 4, 693, 260, 26, 18, 158, 141, 130, 6, 54, 85, 8, 184, 701, 950, 76, 69, 184, 6, 27, 16, 22, 840, 6, 71, 61, 4, 3674, 9, 4, 26, 22, 10, 41, 840, 5, 16, 22, 118, 840, 10, 41, 4203, 14, 15, 12, 13, 14, 15, 12, 13, 4, 78, 290, 18, 164, 36, 1611, 25, 3083, 72, 79, 166, 19, 22, 286, 339, 18, 140, 1196, 16, 6, 38, 1040, 39, 300, 6, 6889, 1449, 60, 33, 439, 57, 531, 244, 339, 7, 16, 21, 36, 856, 154, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tensor, train_labels = features_to_tensor(train_features)\nval_tensor, val_labels = features_to_tensor(val_features)\ntest_tensor, test_labels = features_to_tensor(test_features)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_tensor.size())","execution_count":19,"outputs":[{"output_type":"stream","text":"torch.Size([23750, 200])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(text2id.vocab))","execution_count":20,"outputs":[{"output_type":"stream","text":"10000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset,DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\ntest_loader = DataLoader(TensorDataset(test_tensor,test_labels),batch_size=64)\ntrain_loader = DataLoader(TensorDataset(train_tensor,train_labels),batch_size=64,shuffle = True)\nval_loader = DataLoader(TensorDataset(val_tensor,val_labels),batch_size=64)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for xx,yy in train_loader:\n    print(xx)\n    print(yy)\n    break","execution_count":26,"outputs":[{"output_type":"stream","text":"tensor([[   2,   18,   22,  ..., 1930, 8035,    5],\n        [   2, 1217,    5,  ...,    0,    0,    0],\n        [   2,   18,  158,  ...,    0,    0,    0],\n        ...,\n        [   2,  273,   54,  ...,   35, 4130,   34],\n        [   2,   42,   96,  ...,   42,    6,   16],\n        [   2,   18,  118,  ...,    0,    0,    0]])\ntensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NN(nn.Module):\n    def __init__(self):\n        super(NN, self).__init__()\n        self.layer0 = nn.Embedding(10000,50)\n        self.layer1 = nn.Sequential(\n            nn.Conv1d(in_channels=50, out_channels=100, kernel_size=3,padding=2),\n            nn.ReLU(),\n            nn.MaxPool1d(3),\n            nn.Conv1d(in_channels=100, out_channels=100, kernel_size=3,padding=2),\n            nn.ReLU(),\n            nn.MaxPool1d(3),\n            nn.Conv1d(in_channels=100, out_channels=120, kernel_size=3,padding=2),\n            nn.ReLU(),\n            nn.MaxPool1d(4),)\n        \n        self.layer2 = nn.Sequential(\n            nn.Linear(720,1), nn.Sigmoid()\n        )\n    def forward(self, x):\n        x = self.layer0(x)\n        x = x.transpose(1,2)\n        y = self.layer2(self.layer1(x).view(x.size(0), -1))\n        return y","execution_count":116,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def batch_res(y_pred_all,output):\n    for i in output:\n        if i>0.5:\n            y_pred_all.append(1)\n        else:\n            y_pred_all.append(0)\ndef fit(model, train_dl,val_dl, lr, epoches,tolerance):\n    model.cuda()\n    criterion = nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    best_loss = 100\n    cur_tol = tolerance\n    for epoche in range(epoches):\n        ep_loss = 0\n        for xx,yy in train_dl:\n            xx,yy = xx.cuda(), yy.cuda()\n            optimizer.zero_grad()\n            y_pred = model(xx)\n            loss = criterion(y_pred, yy.float())\n            loss.backward()\n            ep_loss+=loss.item()\n            optimizer.step()\n        with torch.no_grad():\n            val_loss=0\n            for xx,yy in val_dl:\n                xx,yy = xx.cuda(), yy.cuda()\n                y_pred = model(xx)\n                loss = criterion(y_pred, yy.float())\n                val_loss+=loss.item()\n            val_loss/=len(val_dl)\n            if best_loss>= val_loss:\n                best_loss = val_loss\n                cur_tol = tolerance\n                torch.save(model.state_dict(), \"..\\bestmodel.mod\")\n            else:\n                cur_tol -= 1\n            if cur_tol==0:\n                model.load_state_dict(torch.load(\"..\\bestmodel.mod\"))\n                break\n        print(epoche,\"    Loss: {}\".format(ep_loss/len(train_dl)), \"---->Val loss: {}\".format(val_loss))\n    print(\"Best loss is \", best_loss)\n    print(\"Stop train.\")\n    model.cpu()","execution_count":117,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = NN()\nfit(net,train_loader,val_loader,0.005,15,10)","execution_count":118,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\n  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])) is deprecated. Please ensure they have the same size.\n  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([34])) that is different to the input size (torch.Size([34, 1])) is deprecated. Please ensure they have the same size.\n  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n","name":"stderr"},{"output_type":"stream","text":"0     Loss: 0.5119494117235625 ---->Val loss: 0.3479331016540527\n1     Loss: 0.28308007721939393 ---->Val loss: 0.3333878993988037\n2     Loss: 0.20783451081363744 ---->Val loss: 0.37650136426091196\n3     Loss: 0.17892269694036053 ---->Val loss: 0.41734743416309356\n4     Loss: 0.1534567443824183 ---->Val loss: 0.8312504947185516\n5     Loss: 0.09580146791713853 ---->Val loss: 1.1243581086397172\n6     Loss: 0.10376980108639566 ---->Val loss: 0.6242071568965912\n7     Loss: 0.10075629055399889 ---->Val loss: 0.7970703899860382\n8     Loss: 0.06316325119616757 ---->Val loss: 0.9806081369519234\n9     Loss: 0.041872288351402845 ---->Val loss: 0.9513038605451584\n10     Loss: 0.03286363384300315 ---->Val loss: 1.1349822551012039\nBest loss is  0.3333878993988037\nStop train.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"net.load_state_dict(torch.load(\"..\\bestmodel.mod\"))\ny_true = []\ny_pred = []\nfor xx,yy in test_loader:\n    net.cuda()\n    xx,yy = xx.cuda(), yy.cuda()\n    out = net(xx)\n    batch_res(y_pred,out)\n    y_true.extend(yy.tolist())\nnet.cpu()\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_true, y_pred))\n#print(confusion_matrix(y_true, y_pred))","execution_count":119,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.84      0.87      0.85     12500\n           1       0.86      0.83      0.85     12500\n\n   micro avg       0.85      0.85      0.85     25000\n   macro avg       0.85      0.85      0.85     25000\nweighted avg       0.85      0.85      0.85     25000\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}